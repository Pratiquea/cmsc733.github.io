<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>MyAutoPano</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Course materials and notes for University of Maryland's class CMSC733: Computer Vision.">
    <link rel="canonical" href="http://cmsc733.github.io/2019/proj/p1/">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <!-- Google fonts -->
    <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

    <!-- Google tracking -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-46895817-2', 'auto');
      ga('send', 'pageview');

    </script>
    
</head>


    <body>

    <header class="site-header">

  <div class="wrap title-wrap">
    <a class="site-title" href="/">CMSC733 Computer Vision</a>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>MyAutoPano</h1>
  </header>

  <article class="post-content">
  <p>Table of Contents:<br />
- <a href="#due">1. Deadline</a><br />
- <a href="#prob">2. Problem Statement</a><br />
- <a href="#ph1">3. Phase 1: Traditional Approach</a><br />
	- <a href="#corner">3.1. Corner Detection</a><br />
	- <a href="#anms">3.2. Adaptive Non-Maximal Suppression (ANMS)</a><br />
	- <a href="#feat-descriptor">3.3. Feature Descriptor</a><br />
	- <a href="#feat-matching">3.4. Feature Matching</a><br />
	- <a href="#homography">3.5. RANSAC for outlier rejection and to estimate Robust Homography</a><br />
	- <a href="#blending">3.6. Blending Images</a><br />
- <a href="#ph2">4. Phase 2: Deep Learning Approach</a><br />
	- <a href="#datagen">4.1. Data Generation</a><br />
	- <a href="#ph2sup">4.2. Supervised Approach</a><br />
	- <a href="#ph2unsup">4.3. Unsupervised Approach</a><br />
- <a href="#testset">5. Notes about Test Set</a><br />
- <a href="#extra">6. Extra Credit</a><br />
- <a href="#sub">7. Submission Guidelines</a><br />
  - <a href="#files">7.1. File tree and naming</a><br />
  - <a href="#report">7.2. Report</a><br />
- <a href="#coll">8. Collaboration Policy</a></p>

<p><a name="due"></a><br />
## 1. Deadline <br />
<strong>11:59 PM, Thursday, February 21, 2019.</strong></p>

<p><a name="prob"></a><br />
## 2. Problem Statement <br />
The purpose of this project is to stitch two or more images in order to create one seamless panorama image. Each image should have few repeated local features (<script type="math/tex">\sim</script> 30-50\% or more, emperically chosen). In this project, you need to capture multiple such images. The following method of stitching images should work for most image sets but you’ll need to be creative for working on harder image sets.</p>

<p><a name="ph1"></a><br />
## 3. Phase 1: Traditional Approach<br />
An overview of the panorama stitching using the traditional approach is given below.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p1/TraditionalOverview.png" width="100%" />
  <div class="figcaption">
  	Fig 1: Overview of panorama stitching using traditional method.
  </div>
  <div style="clear:both;"></div>
</div>

<p>Sample input images and it’s respective output are shown below.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p1/InputOutput.png" width="100%" />
  <div class="figcaption">
  	Fig 2: First three images: Input to the panorama stitching algorithm, last image: output of the panorama stitching algorithm.
  </div>
  <div style="clear:both;"></div>
</div>

<p><a name="corner"></a><br />
### 3.1. Corner Detection<br />
The first step in stitching a panorama is extracting corners like most computer vision tasks. Here we will use either Harris corners or Shi-Tomasi corners. Refer to <code class="highlighter-rouge">cv2.cornerHarris</code> or <code class="highlighter-rouge">cv2.goodFeaturesToTrack</code> to implement this part.</p>

<p><a name="anms"></a><br />
### 3.2. Adaptive Non-Maximal Suppression (ANMS)<br />
The objective of this step is to detect corners such that they are equally distributed across the image in order to avoid weird artifacts in warping.</p>

<p>In a real image, a corner is never perfectly sharp, each corner might get a lot of hits out of the <script type="math/tex">N</script> strong corners - we want to choose only the <script type="math/tex">N_{best}</script> best corners after ANMS. In essence, you will get a lot more corners than you should! ANMS will try to find corners which are true local maxima. The algorithm for implementing ANMS is given below.</p>

<div class="fig figcenter fighighlight">
  <img src="/assets/2019/p1/anms.png" width="100%" />
  <div class="figcaption">
  	Fig 3: ANMS algorithm.
  </div>
</div>

<p>Output of corner detection and ANMS is shown below. Observe that the output of ANMS is evenly distributed strong corners.</p>

<div class="fig figcenter fighighlight">
  <img src="/assets/2019/p1/ANMSOutput.png" width="100%" />
  <div class="figcaption">
  	Fig 4: Left: Output of corner detection. Right: Output of ANMS.
  </div>
</div>

<p>A sample output of ANMS for two different images are shown below.</p>

<div class="fig figcenter fighighlight">
  <img src="/assets/2019/p1/anms-output.png" width="100%" />
  <div class="figcaption">
  	Output of ANMS for different images.
  </div>
</div>

<p><a name="feat-descriptor"></a><br />
### 3.3. Feature Descriptor<br />
In the previous step, you found the feature points (locations of the <script type="math/tex">N</script> best best corners after ANMS are called the feature point locations). You need to describe each feature point by a feature vector, this is like encoding the information at each feature points by a vector. One of the easiest feature descriptor is described next.</p>

<p>Take a patch of size <script type="math/tex">40 \times 40</script> centered <strong>(this is very important)</strong> around the keypoint. Now apply gaussian blur (feel free to play around with the parameters, for a start you can use OpenCV’s default parameters in <code class="highlighter-rouge">cv2.GaussianBlur</code> command. Now, sub-sample the blurred output (this reduces the dimension) to <script type="math/tex">8 \times 8</script>. Then reshape to obtain a <script type="math/tex">64 \times 1</script> vector. Standardize the vector to have zero mean and variance of 1. Standardization is used to remove bias and to achieve some amount of illumination invariance.</p>

<p><a name="feat-match"></a><br />
### 3.4. Feature Matching<br />
In the previous step, you encoded each keypoint by <script type="math/tex">64\times1</script> feature vector. Now, you want to match the feature points among the two images you want to stitch together. In computer vision terms, this step is called as finding feature correspondences within the 2 images. Pick a point in image 1, compute sum of square difference between all points in image 2. Take the ratio of best match (lowest distance) to the second best match (second lowest distance) and if this is below some ratio keep the matched pair or reject it. Repeat this for all points in image 1. You will be left with only the confident feature correspondences and these points will be used to estimate the transformation between the 2 images also called as Homography. Use the function <code class="highlighter-rouge">cv2.drawMatches</code> to visualize feature correspondences. Below is an image showing matched features.</p>

<div class="fig figcenter fighighlight">
  <img src="/assets/2019/p1/FeatureMatching.png" width="100%" />
  <div class="figcaption">
  	Fig 5: Output of Feature Matching. Observe the wrong matches. 
  </div>
</div>

<p><a name="homography"></a><br />
### 3.5. RANSAC for outlier rejection and to estimate Robust Homography<br />
We now have matched all the features correspondences but not all matches will be right. To remove incorrect matches, we will use a robust method called <em>Random Sampling Concensus</em> or <strong>RANSAC</strong> to compute homography.</p>

<p>Recall the RANSAC steps are: <br />
1. Select four feature pairs (at random), <script type="math/tex">p_i</script> from image 1, <script type="math/tex">p_i^1</script> from image 2.<br />
2. Compute homography <script type="math/tex">H</script> (exact).<br />
3. Compute inliers where <script type="math/tex">% <![CDATA[
SSD(p_i^1, Hp_i) < \texttt{thresh} %]]></script>. <br />
4. Repeat the last three steps until you have exhausted <script type="math/tex">N_{max}</script> number of iterations (specified by user) or you found more than percentage of inliers (Say <script type="math/tex">90\%</script> for example).<br />
5. Keep largest set of inliers.<br />
6. Re-compute least-squares <script type="math/tex">\hat{H}</script> estimate on all of the inliers.</p>

<p>The output of feature matches after all outliers have been removed is shown below.</p>

<div class="fig figcenter fighighlight">
  <img src="/assets/2019/p1/AfterRANSAC.png" width="100%" />
  <div class="figcaption">
  	Fig 6: Feature matches after outliers have been removed using RANSAC. 
  </div>
</div>

<p><a name="blending"></a><br />
### 3.6. Blending Images<br />
Panorama can be produced by overlaying the pairwise aligned images to create the final output image. The output panorama stitched from two images shown in the Fig. 6 are shown below.</p>

<div class="fig figcenter fighighlight">
  <img src="/assets/pano/pano-output.png" width="80%" />
  <div class="figcaption"> Fig 7: Final Panorama output for images shown in Fig. 6. </div>
</div>

<p><em>Come up with a logic to blend the common region between images while not affecting the regions which are not common.</em> Here, common means shared region, i.e., a part of first image and part of second image should overlap in the output panorama. Describe what you did in your report.</p>

<p style="background-color:#ddd; padding:5px"><b>Note:</b> The pipeline talks about how to stitch a pair of images, you need to extend this to work for multiple images. You can re-run your images pairwise or do something smarter.</p>
<p>Your end goal is to be able to stitch any number of given images - maybe 2 or 3 or 4 or 100, your algorithm should work. If a random image with no matches are given, your algorithm needs to report an error.</p>

<p style="background-color:#ddd; padding:5px"><b>Note:</b> When blending these images, there are inconsistency between pixels from different input images due to different exposure/white balance settings or photometric distortions or vignetting. This can be resolved by <i><a href="http://www.irisa.fr/vista/Papers/2003_siggraph_perez.pdf">Poisson blending</a></i>.</p>

<p>An input and output of a seamless panorama of three images are shown below.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p1/InputOutput.png" width="100%" />
  <div class="figcaption">
  	Fig 8: First three images: Input to the panorama stitching algorithm, last image: output of the panorama stitching algorithm.
  </div>
</div>

<p><a name="ph2"></a><br />
## 4. Phase 2: Deep Learning Approach</p>

<p>You are going to be implementing two deep learning approaches to estimate the homography between two images. The deep model effectively combines corner detection, ANMS, feature extraction, feature matching, RANSAC and estimate homography all into one. This not only makes the approach faster but also makes it robust if the network is generalizable (More on this <a href="#ph2unsup">later</a>).</p>

<p><a name="datagen"></a><br />
### 4.1. Data Generation<br />
To train a Convolutional Neural Network (CNN) to estimate homography between a pair of images (we call this network <em>HomographyNet</em> and the original paper can be found <a href="https://arxiv.org/pdf/1606.03798.pdf">here</a>) we need data (pairs of images) with the known homogaraphy between them. This is in-general hard to obtain as we would need the 3D movement between the pair of images to obtain the homography between them. An easier option is to generate synthetic pairs of images to train a network. But what images do we use so that the network is not baised? Simple, use images from <a href="http://cocodataset.org/#home">MSCOCO dataset</a> which contains images of a lot of objects in natural scenes. MSCOCO is quite large and it’ll take forever to train on these images. Hence, we provide a small subset of MSCOCO for you to train your HomographyNet on. This dataset can be downloaded from <a href="https://arxiv.org/pdf/1606.03798.pdf">here</a>.</p>

<p>Now that you’ve downloaded the dataset, we need to generate synthetic data, i.e., pairs of images with known homography between them. Before, we generate image pairs, we need all the image pairs to be of the same size (as HomographyNet is not fully convolutional, it cannot accept image sizes of arbitrary shape). First step in generating data is to obtain a random crop of the image (called patch). Then the original image will be warped using a random homography then the respective patch is extracted. While we perform this operation we need to ensure that we are not extracting the patch from outside the image after warping. An illustration is shown below.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p1/ActiveRegion.png" width="100%" />
  <div class="figcaption">
  	Fig. 9: Patch is shown as dashed blue box. Active region is shown as a blue highlight -- this is the region where the top left corner of the patch can lie such that all the pixels in the patch will lie within the image after warping the random extracted patch. Red line shows the maximum perturbation \(\rho\). 
  </div>
  <div style="clear:both;"></div>
</div>

<p>Let’s go through the steps of generating data now.</p>

<p>In step 1, obtain a random patch (<script type="math/tex">P_A</script> of size <script type="math/tex">M_P\times N_P</script>) from the image (<script type="math/tex">I_A</script>) of size <script type="math/tex">M\times N</script>) with <script type="math/tex">M>M_P</script> and <script type="math/tex">N>N_P</script> such that all the pixels in the patch will lie within the image after warping the random extracted patch. Think about where you have to extract the patch <script type="math/tex">P_A</script> from in <script type="math/tex">I_A</script> if maximum possible perturbation is <script type="math/tex">[-\rho, \rho]</script>.</p>

<p>In step 2, perform a random perturbation in the range <script type="math/tex">[-\rho, \rho]</script> of the corner points (top left corner, top right corner, left bottom corner and right bottom corner – <em>not the corners in computer vision sense</em>) of <script type="math/tex">P_A</script> in <script type="math/tex">I_A</script>. This is illustrated in the figures below.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p1/I1Patch.png" width="100%" />
  <div class="figcaption">
  	Fig. 10: Extracted random patch \(P_A\) from original image \(I_A\) is shown as dashed blue box. Corners of the patch \(P_A\) denoted by \(C_A\) are are shown as blue circles.   
  </div>
  <div style="clear:both;"></div>
</div>

<div class="fig fighighlight">
  <img src="/assets/2019/p1/PerturbPts.png" width="100%" />
  <div class="figcaption">
  	Fig. 11: Random perturbation applied to corners of the patch \(P_A\) denoted by \(C_A\) are shown as blue circles to obtain corners of patch \(P_B\) denoted by \(C_B\) are shown as red circles. 
  </div>
  <div style="clear:both;"></div>
</div>

<div class="fig fighighlight">
  <img src="/assets/2019/p1/PerturbPtsImg.png" width="100%" />
  <div class="figcaption">
  	Fig. 12: Red dashed lines show the patch formed by perturbed point corners. Notice how the shape is not rectangular making it hard to extract data without adding extra data or masking.
  </div>
  <div style="clear:both;"></div>
</div>

<p>As mentioned in the last figure, we want to extract data such that we add minimal amount of extra data and we don’t want to mask anything or add black pixels (pixels outside image assuming zero padding). In order to do this, we’ll warp the image <script type="math/tex">I_A</script> with the inverse of homography between <script type="math/tex">C_A</script> and <script type="math/tex">C_B</script> (denoted by <script type="math/tex">H_B^A</script>), i.e., which is the homography between <script type="math/tex">C_B</script> and <script type="math/tex">C_A</script> (denoted by <script type="math/tex">H_A^B</script>). Refer to <code class="highlighter-rouge">cv2.getPerspectiveTransform</code> and <code class="highlighter-rouge">np.linalg.inv</code> to implement this part.</p>

<p>In step 3, use the value of <script type="math/tex">H_A^B</script> to warp <script type="math/tex">I_A</script> and obtain <script type="math/tex">I_B</script>. Refer  to <code class="highlighter-rouge">cv2.warpPerspective</code> to implement this part. Now, we can extract the patch <script type="math/tex">P_B</script> using the corners in <script type="math/tex">C_A</script> (work the math out and convince yourself why this is true). This is shown in the figure below.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p1/I2Patch.png" width="100%" />
  <div class="figcaption">
  	Fig. 13: Red dashed lines show the patch \(P_B\) extracted from warped image \(I_B\).
  </div>
  <div style="clear:both;"></div>
</div>

<p>Now, the extracted patches are shown below.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p1/Patches.png" width="100%" />
  <div class="figcaption">
  	Fig. 14: Extracted Patches \(P_A\) and \(P_B\) with known homography between them. 
  </div>
  <div style="clear:both;"></div>
</div>

<p>Note that, we generated labels (ground truth homography between the two patches) as is given by <script type="math/tex">H_A^B</script>. However, the authors in <a href="https://arxiv.org/pdf/1606.03798.pdf">this paper</a> found that regressing the 9 values of the homography matrix directly yielded bad results. Instead, another way the authors found was to regress the amount the corners of the patch <script type="math/tex">P_A</script> denoted by <script type="math/tex">C_A</script> need to be moved so that they are aligned with <script type="math/tex">P_B</script>. This is denoted by <script type="math/tex">H_{4Pt}</script> and will be used as our labels. Remember,  <script type="math/tex">H_{4Pt}</script> is given by <script type="math/tex">H_{4Pt} = C_B - C_A</script>.</p>

<p>Now, we stack the image patches <script type="math/tex">P_A</script> and <script type="math/tex">P_B</script> depthwise to obtain an input of size <script type="math/tex">M_P\times N_P \times 2*K</script> where <script type="math/tex">K</script> is the number of channels in each patch/image (3 if RGB image and 1 if grayscale image).</p>

<p>The final output of data generation are these stacked image patches and the homography between them given by <script type="math/tex">H_{4Pt}</script>. An overview of panorama stitching using Deep Learning (both supervised and unsupervised have the same overview) is shown below.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p1/DLOverview.png" width="100%" />
  <div class="figcaption">
  	Fig. 15: Overview of panorama stitching using deep learning based method.
  </div>
  <div style="clear:both;"></div>
</div>

<p><a name="ph2sup"></a><br />
### 4.2. Supervised Approach</p>

<p>The network architecture and the overview is shown below. <em>However, note that you are free to change the network as you wish.</em></p>

<div class="fig fighighlight">
  <img src="/assets/2019/p1/HomographyNetSup.png" width="100%" />
  <div class="figcaption">
  	Fig. 16: Top: Overview of the supervised deep learning system for homography estimation. Bottom: Architecture of the network which mimics the network given in <a href="https://arxiv.org/pdf/1606.03798.pdf">this paper</a>.
  </div>
  <div style="clear:both;"></div>
</div>

<p>The loss function used is simple L2 loss between the predicted 4-point homography <script type="math/tex">\widetilde{H_{4Pt}}</script> and ground truth 4-point homography <script type="math/tex">H_{4Pt}</script>. The loss function <script type="math/tex">l</script> is given by <script type="math/tex">l = \vert \vert \widetilde{H_{4Pt}} - H_{4Pt} \vert \vert_2</script>.</p>

<p><a name="ph2unsup"></a><br />
### 4.3. Unsupervised Approach<br />
Though the supervised deep homography estimation works well, it needs a lot of data to generalize well and is generally not robust if good data augmentation is not provided as shown in <a href="https://arxiv.org/abs/1709.03966">this paper</a>. Now, we’ll implement an unsupervised approach to estimate homography between image pairs using a CNN as given in <a href="https://arxiv.org/abs/1709.03966">this paper</a>.</p>

<p>The data generation is exactly the same for this part (as we want to evaluate the algorithm later using the labels), note that the labels <script type="math/tex">H_{4Pt}</script> are not used for learning in the loss function. For the unsupervised method, we want to predict the value of <script type="math/tex">H_{4Pt}</script> which when used to warp the patch <script type="math/tex">P_A</script> should look exactly like the patch <script type="math/tex">P_B</script>. This can be written as the photometric loss function <script type="math/tex">l</script> given by <script type="math/tex">l = \vert \vert w(P_A, H_{4Pt}) - P_B\vert \vert_1</script>. Here <script type="math/tex">w(\cdot)</script> represents a generic warping function and here specifically warping using the Homography value and bilinear interpolation. Let’s see how we can convert our supervised method into unsupervised.</p>

<div class="fig fighighlight">
  <img src="/assets/2019/p1/HomographyNetUnsup.png" width="100%" />
  <div class="figcaption">
    Fig. 17: Overview of the unsupervised deep learning system for homography estimation.
  </div>
  <div style="clear:both;"></div>
</div>

<p>Notice that, now we have two new parts (shown in light blue and yellow), namely, TensorDLT and Spatial Transformer Network. The TensorDLT part takes as input the predicted 4-point homography <script type="math/tex">\widetilde{H_{4Pt}}</script> and corners of patch <script type="math/tex">P_A</script> to estimate the full <script type="math/tex">3\times 3</script> estimated homgraphy matrix <script type="math/tex">\widetilde{\mathbf{H}}</script> which is used to warp the patch <script type="math/tex">P_A</script> using a differentiable warping layer (using bilinear interpolation) called the Spatial Transformer Network. To implement the TensorDLT layer, refer to <a href="https://arxiv.org/abs/1709.03966">Section IV B in this paper</a>. A generic Spatial Transformer Network is given to you with the starter code (you need to modify it as descibed in <a href="https://arxiv.org/abs/1709.03966">Section IV C of this paper</a>). Finally, implement the photometric loss function we discussed earlier.</p>

<p>Note that, we didn’t talk about the network architecture here, feel free to use the network you implemented for the supervised version here or be creative.</p>

<p><a name="testset"></a><br />
## 5. Notes about Test Set<br />
One day (24 hours) before the deadline, a test set will be released on which we expect you to run your code from both the parts and present the results in a report (more on this <a href="#sub">later</a>). For the deep learning part, your algorithm will only run on the image size you chose during training, i.e., <script type="math/tex">M_P\times N_P</script>. A simple way to deal with this is to resize the test image to <script type="math/tex">M_P\times N_P</script> to obtain the homography and then warp the original image or crop a central region of <script type="math/tex">M_P\times N_P</script> or obtain random crops of size <script type="math/tex">M_P\times N_P</script> and average all the predicted homography values. <em>Feel free to be creative here.</em></p>

<p><a name="extra"></a><br />
## 6. Extra Credit<br />
Implementing the extra credit can give you upto 25% on the bonus score. As we discussed <a href="#testset">earlier</a>, there is no optimal way to obtain the best homography between two images as we trained our networks on a small patch size. A good way would be to obtain homographies from a lot of patches and then use RANSAC to obtain the best method. We want you to implement this method using deep learning and present a high quality analysis. Refer to the <a href="https://arxiv.org/abs/1611.05705">DSAC paper</a> which presents ideas on how to implement RANSAC in a differentiable way for a neural network. You are free to use the DSAC <a href="https://hci.iwr.uni-heidelberg.de/vislearn/research/scene-understanding/pose-estimation/#DSAC">code from the authors</a> to implement this part.</p>

<p><a name="sub"></a><br />
## 7. Submission Guidelines</p>

<p><b> If your submission does not comply with the following guidelines, you’ll be given ZERO credit </b></p>

<p><a name="files"></a><br />
### 7.1. File tree and naming</p>

<p>Your submission on ELMS/Canvas must be a <code class="highlighter-rouge">zip</code> file, following the naming convention <code class="highlighter-rouge">YourDirectoryID_p1.zip</code>. If you email ID is <code class="highlighter-rouge">abc@umd.edu</code> or <code class="highlighter-rouge">abc@terpmail.umd.edu</code>, then your <code class="highlighter-rouge">DirectoryID</code> is <code class="highlighter-rouge">abc</code>. For our example, the submission file should be named <code class="highlighter-rouge">abc_p1.zip</code>. The file <strong>must have the following directory structure</strong> because we’ll be autograding assignments. The file to run for your project should be called <code class="highlighter-rouge">Wrapper.py</code>. You can have any helper functions in sub-folders as you wish, be sure to index them using relative paths and if you have command line arguments for your Wrapper codes, make sure to have default values too. Please provide detailed instructions on how to run your code in <code class="highlighter-rouge">README.md</code> file. Please <strong>DO NOT</strong> include data in your submission.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>YourDirectoryID_p1.zip
│   README.md
|   Your Code files 
|   ├── Any subfolders you want along with files
|   Wrapper.py 
└── Report.pdf
</code></pre>
</div>
<p><a name="report"></a><br />
### 7.2. Report</p>

<p>For each section of the project, explain briefly what you did, and describe any interesting problems you encountered and/or solutions you implemented.  You must include the following details in your writeup:</p>

<ul>
  <li>Your report <strong>MUST</strong> be typeset in LaTeX in the IEEE Tran format provided to you in the <code class="highlighter-rouge">Draft</code> folder and should of a conference quality paper.</li>
  <li>For Phase 1, present input and output images after each section (corner detection, ANMS, feature extraction, feature matching, feature matches after RANSAC) and final panorama.</li>
  <li>For Phase 2, present supervised and unsupervised detected homographies against a synthetic ground truth for 4 images.</li>
  <li>For Phase 2, present input and output panorama’s using supervised and unsupervised approaches.</li>
  <li>For Phase 2, present the average EPE results for both supervised and unsupervised approaches along with agorithm run-time for forward pass of the network.</li>
</ul>

<p><a name="coll"></a><br />
## Collaboration Policy<br />
You are encouraged to discuss the ideas with your peers. However, the code should be your own, and should be the result of you exercising your own understanding of it. If you reference anyone else’s code in writing your project, you must properly cite it in your code (in comments) and your writeup. For the full honor code refer to the CMSC733 Spring 2019 website.</p>


  </article>

</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <div class="footer-col-1 column">
      <ul>
        
        
        
        <li>
          <a href="mailto:"></a>
        </li>
      </ul>
    </div>

    <div class="footer-col-2 column">
        
    </div>

    <div class="footer-col-3 column">
      
    </div>

  </div>

</footer>


    <!-- mathjax -->
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </body>
</html>
